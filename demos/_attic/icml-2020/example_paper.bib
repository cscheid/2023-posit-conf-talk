
@inproceedings{ribeiro2016should,
  title={Why should I trust you?: Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016},
  organization={ACM}
}

@article{owen2017shapley,
  title={On Shapley value for measuring importance of dependent inputs},
  author={Owen, Art B and Prieur, Cl{\'e}mentine},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={5},
  number={1},
  pages={986--1002},
  year={2017},
  publisher={SIAM}
}

@article{lundberg2018consistent,
  title={Consistent individualized feature attribution for tree ensembles},
  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},
  journal={arXiv preprint arXiv:1802.03888},
  year={2018}
}

@article{lipovetsky2001analysis,
  title={Analysis of regression in game theory approach},
  author={Lipovetsky, Stan and Conklin, Michael},
  journal={Applied Stochastic Models in Business and Industry},
  volume={17},
  number={4},
  pages={319--330},
  year={2001},
  publisher={Wiley Online Library}
}

@inproceedings{datta2016algorithmic,
  title={Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems},
  author={Datta, Anupam and Sen, Shayak and Zick, Yair},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={598--617},
  year={2016},
  organization={IEEE}
}

@inproceedings{ustun2019actionable,
  title={Actionable recourse in linear classification},
  author={Ustun, Berk and Spangher, Alexander and Liu, Yang},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={10--19},
  year={2019},
  organization={ACM}
}

@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={Calif. L. Rev.},
  volume={104},
  pages={671},
  year={2016},
  publisher={HeinOnline}
}

@article{angwin,
title={How we analyzed the COM- PAS recidivism algorithm},
author={Larson, Jeff and Mattu, Surya and Kirchner, Lauren and Angwin, Julia},
journal={Propublica},
year = {2016},
url = {https://www.propublica. org/article/how-we-analyzed-the-compas-recidivism-algorithm}
}

@article{verge,
title = {Why the future of life insurance may depend on your online presence},
author = {Chen, Angela},
journal={The Verge},
year = {2019},
url = {https://www.theverge.com/2019/2/7/18211890/social-media-life-insurance-new-york-algorithms-big-data-discrimination-online-records}}

@inproceedings{kleinberg2017inherent,
  title={Inherent Trade-Offs in the Fair Determination of Risk Scores},
  author={Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  booktitle={8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  year={2017},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@article{calders2010three,
  title={Three naive Bayes approaches for discrimination-free classification},
  author={Calders, Toon and Verwer, Sicco},
  journal={Data Mining and Knowledge Discovery},
  volume={21},
  number={2},
  pages={277--292},
  year={2010},
  publisher={Springer}
}

@inproceedings{zafar2017fairness,
  title={Fairness Constraints: Mechanisms for Fair Classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P},
  booktitle={Artificial Intelligence and Statistics},
  pages={962--970},
  year={2017}
}

@article{chouldechova2017fair,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Chouldechova, Alexandra},
  journal={Big data},
  volume={5},
  number={2},
  pages={153--163},
  year={2017},
  publisher={Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA}
}

@inproceedings{feldman2015certifying,
  title={Certifying and removing disparate impact},
  author={Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={259--268},
  year={2015},
  organization={ACM}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4765--4774},
  year={2017}
}

@article{SlackHilgard2020FoolingLIMESHAP, title={Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods}, author={Dylan Slack and Sophie Hilgard and Emily Jia and Sameer Singh and Himabindu Lakkaraju}, journal={AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES)}, year={2020}, }

@article{vstrumbelj2014explaining,
  title={Explaining prediction models and individual predictions with feature contributions},
  author={{\v{S}}trumbelj, Erik and Kononenko, Igor},
  journal={Knowledge and information systems},
  volume={41},
  number={3},
  pages={647--665},
  year={2014},
  publisher={Springer}
}

@article{adler2018auditing,
  title={Auditing black-box models for indirect influence},
  author={Adler, Philip and Falk, Casey and Friedler, Sorelle A and Nix, Tionney and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
  journal={Knowledge and Information Systems},
  volume={54},
  number={1},
  pages={95--122},
  year={2018},
  publisher={Springer}
}

@inproceedings{datta2016indirect,
  title={Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems},
  author={Datta, Anupam and Sen, Shayak and Zick, Yair},
  booktitle={2016 IEEE Symposium on Security and Privacy (SP)},
  pages={598--617},
  year={2016},
  organization={IEEE}
}

@article{reid2016study,
  title={A study of error variance estimation in lasso regression},
  author={Reid, Stephen and Tibshirani, Robert and Friedman, Jerome},
  journal={Statistica Sinica},
  pages={35--67},
  year={2016},
  publisher={JSTOR}
}

@article{moisen2008classification,
  title={Classification and regression trees},
  author={Moisen, GG},
  journal={In: J{\o}rgensen, Sven Erik; Fath, Brian D.(Editor-in-Chief). Encyclopedia of Ecology, volume 1. Oxford, UK: Elsevier. p. 582-588.},
  pages={582--588},
  year={2008}
}

@article{diakopoulos2016accountability,
  title={Accountability in algorithmic decision making},
  author={Diakopoulos, Nicholas},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={56--62},
  year={2016},
  publisher={ACM}
}

@book{pasquale2015black,
  title={The Black Box Society},
  author={Pasquale, Frank},
  year={2015},
  publisher={Harvard University Press}
}

@article{poursabzi2018manipulating,
  title={Manipulating and measuring model interpretability},
  author={Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach, Hanna},
  journal={arXiv preprint arXiv:1802.07810},
  year={2018}
}

@article{kroll2016accountable,
  title={Accountable algorithms},
  author={Kroll, Joshua A and Barocas, Solon and Felten, Edward W and Reidenberg, Joel R and Robinson, David G and Yu, Harlan},
  journal={U. Pa. L. Rev.},
  volume={165},
  pages={633},
  year={2016},
  publisher={HeinOnline}
}


@article{pleasestop,
  title={Please Stop Permuting Features: An Explanation and Alternatives},
  author={Hooker, Giles and Mentch, Lucas},
  journal={arXiv preprint arXiv:1905.03151v1},
  year={2019}
}

@article{aas_explaining_2019,
	title = {Explaining individual predictions when features are dependent: {More} accurate approximations to {Shapley} values},
	shorttitle = {Explaining individual predictions when features are dependent},
	url = {http://arxiv.org/abs/1903.10464},
	abstract = {Explaining complex or seemingly simple machine learning models is a practical and ethical question, as well as a legal issue. Can I trust the model? Is it biased? Can I explain it to others? We want to explain individual predictions from a complex machine learning model by learning simple, interpretable explanations. Of existing work on interpreting complex models, Shapley values is regarded to be the only model-agnostic explanation method with a solid theoretical foundation. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes independent features, which may give very wrong explanations. This is the case even if a simple linear model is used for predictions. We extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with linear and non-linear feature dependence, where our method gives more accurate approximations to the true Shapley values. We also propose a method for aggregating individual Shapley values, such that the prediction can be explained by groups of dependent variables.},
	urldate = {2019-08-25},
	journal = {arXiv:1903.10464 [cs, stat]},
	author = {Aas, Kjersti and Jullum, Martin and Løland, Anders},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.10464},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, shap},
	file = {arXiv\:1903.10464 PDF:/Users/indrakumar/Zotero/storage/XHQBHAF9/Aas et al. - 2019 - Explaining individual predictions when features ar.pdf:application/pdf;arXiv.org Snapshot:/Users/indrakumar/Zotero/storage/JMMY5S6Q/1903.html:text/html}
}

@techreport{selbst_intuitive_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Intuitive} {Appeal} of {Explainable} {Machines}},
	url = {https://papers.ssrn.com/abstract=3126971},
	abstract = {Algorithmic decision-making has become synonymous with inexplicable decision-making, but what makes algorithms so difficult to explain? This Article examines what sets machine learning apart from other ways of developing rules for decision-making and the problem these properties pose for explanation. We show that machine learning models can be both inscrutable and nonintuitive and that these are related, but distinct, properties.Calls for explanation have treated these problems as one and the same, but disentangling the two reveals that they demand very different responses. Dealing with inscrutability requires providing a sensible description of the rules; addressing nonintuitiveness requires providing a satisfying explanation for why the rules are what they are. Existing laws like the Fair Credit Reporting Act (FCRA), the Equal Credit Opportunity Act (ECOA), and the General Data Protection Regulation (GDPR), as well as techniques within machine learning, are focused almost entirely on the problem of inscrutability. While such techniques could allow a machine learning system to comply with existing law, doing so may not help if the goal is to assess whether the basis for decision-making is normatively defensible.In most cases, intuition serves as the unacknowledged bridge between a descriptive account and a normative evaluation. But because machine learning is often valued for its ability to uncover statistical relationships that defy intuition, relying on intuition is not a satisfying approach. This Article thus argues for other mechanisms for normative evaluation. To know why the rules are what they are, one must seek explanations of the process behind a model’s development, not just explanations of the model itself.},
	language = {en},
	number = {ID 3126971},
	urldate = {2019-09-27},
	institution = {Social Science Research Network},
	author = {Selbst, Andrew D. and Barocas, Solon},
	month = mar,
	year = {2018},
	keywords = {explanation talk, algorithmic accountability, big data, discrimination, explanations, law and technology, machine learning, privacy},
	file = {Snapshot:/Users/indrakumar/Zotero/storage/CXVGGIH4/papers.html:text/html}
}

@article{poursabzi-sangdeh_manipulating_2018,
	title = {Manipulating and {Measuring} {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1802.07810},
	abstract = {Despite a growing literature on creating interpretable machine learning methods, there have been few experimental studies of their effects on end users. We present a series of large-scale, randomized, pre-registered experiments in which participants were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Participants who were shown a clear model with a small number of features were better able to simulate the model's predictions. However, contrary to what one might expect when manipulating interpretability, we found no significant difference in multiple measures of trust across conditions. Even more surprisingly, increased transparency hampered people's ability to detect when a model has made a sizeable mistake. These findings emphasize the importance of studying how models are presented to people and empirically verifying that interpretable models achieve their intended effects on end users.},
	urldate = {2019-09-27},
	journal = {arXiv:1802.07810 [cs]},
	author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G. and Hofman, Jake M. and Vaughan, Jennifer Wortman and Wallach, Hanna},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.07810},
	keywords = {Computer Science - Artificial Intelligence, explanation talk, Computer Science - Computers and Society},
	file = {arXiv\:1802.07810 PDF:/Users/indrakumar/Zotero/storage/76IUJVTA/Poursabzi-Sangdeh et al. - 2018 - Manipulating and Measuring Model Interpretability.pdf:application/pdf;arXiv.org Snapshot:/Users/indrakumar/Zotero/storage/X654MU5S/1802.html:text/html}
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	urldate = {2019-09-27},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, explanation talk},
	file = {arXiv\:1702.08608 PDF:/Users/indrakumar/Zotero/storage/6IICWI2X/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf:application/pdf;arXiv.org Snapshot:/Users/indrakumar/Zotero/storage/IVEKEAST/1702.html:text/html}
}

@inproceedings{bhatt_explainable_2019, author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter}, title = {Explainable Machine Learning in Deployment}, year = {2020}, isbn = {9781450369367}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3351095.3375624}, doi = {10.1145/3351095.3375624}, booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency}, pages = {648–657}, numpages = {10}, keywords = {explainability, transparency, deployed systems, machine learning, qualitative study}, location = {Barcelona, Spain}, series = {FAT* ’20} }

@book{osborne1994course,
  title={A course in game theory},
  author={Osborne, Martin J and Rubinstein, Ariel},
  year={1994}
}


@article{passi_trust_2018,
	title = {Trust in {Data} {Science}: {Collaboration}, {Translation}, and {Accountability} in {Corporate} {Data} {Science} {Projects}},
	volume = {2},
	issn = {2573-0142},
	shorttitle = {Trust in {Data} {Science}},
	url = {http://doi.acm.org/10.1145/3274405},
	doi = {10.1145/3274405},
	abstract = {The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.},
	number = {CSCW},
	urldate = {2019-10-28},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Passi, Samir and Jackson, Steven J.},
	month = nov,
	year = {2018},
	keywords = {collaboration, credibility, data science, organizational work, trust},
	pages = {136:1--136:28}
}

@article{miller2019explanation,
  title={Explanation in artificial intelligence: Insights from the social sciences},
  author={Miller, Tim},
  journal={Artificial Intelligence},
  volume={267},
  pages={1--38},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{mittelstadt2019explaining,
  title={Explaining explanations in AI},
  author={Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={279--288},
  year={2019},
  organization={ACM}
}

@article{merrick2019explanation,
  title={The Explanation Game: Explaining Machine Learning Models with Cooperative Game Theory},
  author={Merrick, Luke and Taly, Ankur},
  journal={arXiv preprint arXiv:1909.08128},
  year={2019}
}


@article{wachter2017counterfactual,
  title={Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harv. JL \& Tech.},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}



@article{kruskal1987relative,
  title={Relative importance by averaging over orderings},
  author={Kruskal, William},
  journal={The American Statistician},
  volume={41},
  number={1},
  pages={6--10},
  year={1987},
  publisher={Taylor \& Francis}
}
@article{lipton1990contrastive,
  title={Contrastive explanation},
  author={Lipton, Peter},
  journal={Royal Institute of Philosophy Supplements},
  volume={27},
  pages={247--266},
  year={1990},
  publisher={Cambridge University Press}
}

@article{kahneman1986norm,
  title={Norm theory: Comparing reality to its alternatives.},
  author={Kahneman, Daniel and Miller, Dale T},
  journal={Psychological review},
  volume={93},
  number={2},
  pages={136},
  year={1986},
  publisher={American Psychological Association}
}

@inproceedings{10.1145/3351095.3372830,
author = {Barocas, Solon and Selbst, Andrew D. and Raghavan, Manish},
title = {The Hidden Assumptions behind Counterfactual Explanations and Principal Reasons},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372830},
doi = {10.1145/3351095.3372830},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {80–89},
numpages = {10},
location = {Barcelona, Spain},
series = {FAT* ’20}
}
  
@inproceedings{10.1145/3351095.3372876,
author = {Venkatasubramanian, Suresh and Alfano, Mark},
title = {The Philosophical Basis of Algorithmic Recourse},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372876},
doi = {10.1145/3351095.3372876},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {284–293},
numpages = {10},
keywords = {algorithmic decision making, precarity, robust goods, recourse},
location = {Barcelona, Spain},
series = {FAT* ’20}
}
  

@misc{frye2019asymmetric,
    title={Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability},
    author={Christopher Frye and Ilya Feige and Colin Rowat},
    year={2019},
    eprint={1910.06358},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}



@inproceedings{kaur2019interpreting,
author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
title = {Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376219},
doi = {10.1145/3313831.3376219},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {machine learning, interpretability, user-centric evaluation},
location = {Honolulu, HI, USA},
series = {CHI ’20}
}



@inproceedings{weerts2019human,
author = {Weerts, Hilde J.P. and van Ipenburg, Werner and Pechenizkiy,  Mykola},
title = {A Human-Grounded Evaluation of SHAP for Alert Processing},
year = {2019},
url = {https://arxiv.org/abs/1907.03324},
booktitle = {KDD workshop on Explainable AI (KDD-XAI '19')},
}
